{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BI-RNN.ipynb",
      "provenance": [],
      "mount_file_id": "1pbqlrOJqsXr2uUCEmhiBaUBwSPRX-69c",
      "authorship_tag": "ABX9TyMyzCHPpuf9sA79moOSF9Kv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbrachemi/Bi-RNN-for-IQA/blob/main/BI_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DEPENDENCIES"
      ],
      "metadata": {
        "id": "Bk__2PFA7G3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install slidingwindow"
      ],
      "metadata": {
        "id": "p7M72HmI7JVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca062fa-0fbc-47f2-8a34-c32d6740d985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting slidingwindow\n",
            "  Downloading slidingwindow-0.0.14-py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from slidingwindow) (1.21.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from slidingwindow) (5.4.8)\n",
            "Installing collected packages: slidingwindow\n",
            "Successfully installed slidingwindow-0.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77sx7kWPXiGt"
      },
      "source": [
        "#**IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import slidingwindow as sw\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "import h5py\n",
        "\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "\n",
        "import datetime"
      ],
      "metadata": {
        "id": "parzRAWW80uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyGFr0zqXiGu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import applications \n",
        "import PIL\n",
        "from PIL import Image\n",
        "import os,sys\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras import __version__\n",
        "import pandas\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau ,Callback,TensorBoard,TerminateOnNaN\n",
        "from tensorflow.keras.models import load_model\n",
        "from scipy.stats import spearmanr,pearsonr,kendalltau\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.image import extract_patches_2d\n",
        "import sklearn.datasets\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import MaxPooling2D ,Dense ,Dropout ,Input,concatenate,Conv2D,Reshape,GlobalMaxPooling2D,Flatten,GlobalAveragePooling2D,AveragePooling2D,Lambda,MaxPooling2D\n",
        "from tensorflow.keras.models import Model \n",
        "from sklearn import preprocessing\n",
        "import imutils\n",
        "import random\n",
        "from skimage.feature import hog\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "from keras.activations import softmax,sigmoid\n",
        "import cv2\n",
        "from tensorflow.keras import layers\n",
        "from keras.activations import softmax,sigmoid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.util import view_as_windows\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA"
      ],
      "metadata": {
        "id": "TJ--yplm7hqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the datasets are available via the following links.\n",
        "Please make sure to create two pickle files 'IDs_train.pickle' and 'IDs_test.pickle' containing the train and test IDs and a 'scores.pickle' file containing a dictionnary that maps each id to their MOS/DMOS score. "
      ],
      "metadata": {
        "id": "stoDKBB_CxC2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vS-jAx265S-"
      },
      "outputs": [],
      "source": [
        "############## You can download the datasets by uncommenting the following commands #####################\n",
        "\n",
        "\n",
        "####### TID-2013 #######\n",
        "# !wget \"http://www.ponomarenko.info/tid2013/tid2013.rar\"\n",
        "# !pip install unrar\n",
        "# !unrar x \"/content/tid2013.rar\"\n",
        "\n",
        "\n",
        "####### KADID-10K #######\n",
        "# !wget \"https://datasets.vqa.mmsp-kn.de/archives/kadid10k.zip\"\n",
        "# !unzip /content/kadid10k.zip\n",
        "\n",
        "####### CSIQ #######\n",
        "# !wget \"http://vision.eng.shizuoka.ac.jp/csiq/dst_imgs.zip\"\n",
        "# !unzip /content/dst_imgs.zip\n",
        "\n",
        "####### LIVEMD #######\n",
        "# !wget https://public.boxcloud.com/d/1/b1!IU4S1kNcRl9668x9nt0yijL48I6EGcI3qmccUX2YNXfVw4O2LNS4fEyI3x5aNXOL2OZWHNt-Z7vTEijwPWtsasa8_P2sdaE44u-7QR1N6cNOC3afB8Szq4biRIvtRNmLTnom6NZfdFNQSMbjG6g2yTbPpRoE1YuEGIT648tUedT_eDMHGEPDyINX4hOPrRV1CvIDYMqR4K7Oa0TrM689E8nF-RDRTH2ijx0PSDc84TxdORQ79XRpIq59K3-1OEkLvnDrpcPLxsZXiZAHNjrjggCjYNscJ83COC3_JUWgR6RQ_GpvoyB_60ba1b6o76mQ1UbFRnJ3snPCEuTxb_396uRdq4tEWrnf4G-dn5NKLdvofFiuXfFFEssLoRk3beeY10EuU7z-z6w2sB_3bgJnMysFwUleBBmEgk7zbizL6rtqZ6jxcRhzmGFD7JubS8sP_nQOrIo9JMbf95oMIfLQsom7A1LlgoSyeHJ23QTQuS1Syzjo7_iHE98jBJV3LgSRGsRLPCLfgbCEFmAQEWZ5qIHETx9FEsHtPMrCB8elqLLpfzYbRf5yq8_75sM7pn_Z2ardTDAOa_Uot-nP_rqVMTCHcJSjDPygX7wNwiGITIIQZrL5zX5LUXzCmRdGjgOCuATUIOKniWKrPRmd4lowJ5kJMHM4Gd87OifblNSMxxH1jyaViMT9z5cH8kPl2Eybl3SHTsmbiernufEAnKBaHk4YjEFFTgDaNMklhclyobRNiXJaem5IiO7qmgNhgOhbT0yAYPswQA19ufH0YHzEdEuXuQ1GW2ZqUsY-kojA_0Lz2kt-kWYgIUG8UpopB_YuB64n96icArMIbbDKdbXnsZVu0myUuEMOTkgMTWKsiCYsinvBwOD4b0l1kDtkBAsQToEU34us_xyloT_A1PoV_E6bKJPTdeaTpryWWe-RSM6T5eVnKWlRZqa73-T5WqMIkGLFTdRWJTGs1VlT_GZdFCqPR-VLg6jYvZQ7ju7Bps9defkzJ-r5VmZFEV6LFultyy2tUIEaNvRi6V1YNBHRGfTh-KBaN00B6wNaQcM1FprBRiTN6jixzInXuKTFTOnS7KxiarHX_7x1P19qlf9XU61puHYJIsTQU5CyjA2n5c__JpyQIx94cOykBtle4tA88m8V4CGBTFsNRqZoNpCC1rqh-r-gTiQKz7Tbc8ea4MQiIVd0TUAKFiXge2Thczjgl-_n0nD3M7dO3FiOZvAQNUxEonDGfjsoYGtMNHgoKQO6fH2y68joH26PDr0bdeSdW820M0N50DXIu4p8OJZRNB8mWcGqrCDWXEjq/download\n",
        "# !unrar x /content/drive/MyDrive/download -plivemultidistortiondatabase2013\n",
        "# mkdir /content/livemd\n",
        "# !mv /content/livemd/To_Release/Part1/blurjpeg/* /content/livemd/\n",
        "# !mv /content/livemd/To_Release/Part2/blurnoise/* /content/livemd/\n",
        "\n",
        "####### KONIQ #######\n",
        "# !wget http://datasets.vqa.mmsp-kn.de/archives/koniq10k_1024x768.zip\n",
        "# !unzip koniq10k_1024x768.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwdAlUbP2bo1"
      },
      "source": [
        "#FEATURES EXTRACTION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CNN model"
      ],
      "metadata": {
        "id": "PSP1L7nnERkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Base_Model(base,weights='imagenet', include_top=False, input_shape=(224, 224, 3)):\n",
        "    if(base=='resnet'):\n",
        "        return ResNet50(weights=weights, include_top=include_top, input_shape=input_shape)\n",
        "    if(base=='vgg16'):\n",
        "        return VGG16(weights=weights, include_top=include_top, input_shape=input_shape)\n",
        "    if(base=='inception'):\n",
        "        return InceptionV3(weights=weights, include_top=include_top, input_shape=input_shape)"
      ],
      "metadata": {
        "id": "TgFHQMyrETHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA GENERATOR"
      ],
      "metadata": {
        "id": "RKlBwCxQ-auC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfUgipVC3oLH"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, batch_size=1, dim=(299,299), n_channels=3,n_output=1,\n",
        "                  shuffle=False,base='vgg16',list_IDs_path='',labels_path='',\n",
        "                 part='train',db_path='',overlapping=0.5):\n",
        "      \n",
        "        'Initialization'\n",
        "        self.base=base\n",
        "        self.list_IDs_temp=[]\n",
        "        self.overlapping=overlapping\n",
        "        self.dim = dim\n",
        "        self.db_path=db_path\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs,self.labels = self.load_pkl(list_IDs_path,labels_path,part)\n",
        "        self.n_channels = n_channels\n",
        "        self.n_output = n_output\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def load_pkl(self,list_IDs_path,labels_path,part):    \n",
        "        list_IDs_path=list_IDs_path.replace('.pickle','')\n",
        "        list_IDs_path=list_IDs_path+\"_\"+str(part)+\".pickle\"\n",
        "        pickle_in = open(list_IDs_path,'rb')\n",
        "        list_IDs= pickle.load(pickle_in)\n",
        "        pickle_in.close()\n",
        "        pickle_in2 = open(labels_path,'rb')\n",
        "        labels = pickle.load(pickle_in2)\n",
        "        pickle_in2.close()\n",
        "        list_IDs=list(list_IDs)\n",
        "        return  list_IDs,labels\n",
        "    \n",
        "    \n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs)/ self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # Find list of IDs\n",
        "        self.list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        \n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(self.list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        n=self.dim[0]\n",
        "        'Generates data containing batch_size samples'\n",
        "        \n",
        "        # Initialization\n",
        "        \n",
        "        img = image.load_img(os.path.join(self.db_path,list_IDs_temp[0]))\n",
        "        img = image.img_to_array(img)\n",
        "        windows = sw.generate(img, sw.DimOrder.HeightWidthChannel,n,self.overlapping)\n",
        "        self.patches=len(windows)\n",
        "        \n",
        "        global p\n",
        "        p=self.patches\n",
        "\n",
        "        X = np.empty((self.batch_size,len(windows),*self.dim,self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=np.float32)\n",
        "\n",
        "        \n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            img = image.load_img(self.db_path+ID)\n",
        "            img = image.img_to_array(img)\n",
        "            \n",
        "            windows = sw.generate(img, sw.DimOrder.HeightWidthChannel, n,self.overlapping)\n",
        "            y[i] = self.labels[ID]\n",
        "                   \n",
        "            for k,window in enumerate(windows):\n",
        "                subset = img[ window.indices()]  \n",
        "\n",
        "                if self.base=='vgg16':\n",
        "                   X[i,k,:,:,:]=keras.applications.vgg16.preprocess_input(subset)\n",
        "                if self.base=='inception':\n",
        "                   X[i,k,:,:,:]=keras.applications.inception_v3.preprocess_input(subset)\n",
        "                if self.base=='resnet':\n",
        "                   X[i,k,:,:,:]=keras.applications.resnet.preprocess_input(subset)\n",
        "              \n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiation of the data generator:"
      ],
      "metadata": {
        "id": "LesuYxnBB_E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_generator = DataGenerator(list_IDs_path='./IDs.pickle',overlapping=0,labels_path='./scores.pickle',\n",
        "                     db_path='./Koniq/512x384/',batch_size=1,dim=(224,224), n_channels=3,\n",
        "                     n_output=1, shuffle=False, part='train',base='resnet')"
      ],
      "metadata": {
        "id": "-1bEQIiC_eHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_generator = DataGenerator(list_IDs_path='./IDs.pickle',overlapping=0,labels_path='./scores.pickle',\n",
        "                     db_path='./Koniq/512x384/',batch_size=1,dim=(224,224), n_channels=3,\n",
        "                     n_output=1, shuffle=False, part='test',base='resnet')"
      ],
      "metadata": {
        "id": "DBMyTOHJEBsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features extraction:"
      ],
      "metadata": {
        "id": "K1DHxgo8FmY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model =  Base_Model('resnet',weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
      ],
      "metadata": {
        "id": "GtpieB3XFrli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out=base_model.layers[-1].output\n",
        "out=GlobalAveragePooling2D()(out)\n",
        "model=keras.Model(inputs=base_model.layers[0].output,outputs=out)\n",
        "\n",
        "input=Input(shape=(p,224,224,3))\n",
        "x2= layers.TimeDistributed(model)(input)\n",
        "\n",
        "model_cnn=keras.Model(inputs=input,outputs=x2)     "
      ],
      "metadata": {
        "id": "2TatgiYHFpaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = model_cnn.predict_generator(generator=training_generator)\n",
        "X_test = model_cnn.predict_generator(generator=val_generator)\n",
        "\n",
        "#Uncomment the following lines in order to save the features for further use\n",
        "#h5f = h5py.File('features.h5', 'w')\n",
        "#h5f.create_dataset('X_train', data=X_train)\n",
        "#h5f.create_dataset('X_test', data=X_test)\n",
        "#h5f.close()"
      ],
      "metadata": {
        "id": "pmNBVasNGPsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODEL DEFINITION"
      ],
      "metadata": {
        "id": "tmYKhg0hHT7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return K.sum(output,axis=1)\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ],
      "metadata": {
        "id": "FuFxv9AfHXEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(Layer):\n",
        "    \n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "        super(Attention,self).__init__()\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
        "                               initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
        "                               initializer=\"zeros\")\n",
        "        \n",
        "        super(Attention,self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x*a\n",
        "        \n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "        \n",
        "        return K.sum(output, axis=1)"
      ],
      "metadata": {
        "id": "ClQ6DDUDMCnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_model(dropOutRate=0.25,lstm_layers=2,lstm_units=128,hidden_units=512,num_layers=3,att=True):\n",
        "       \n",
        "   \n",
        "    x2_i=Input(shape=(p,input))\n",
        "    x2_=x2_i\n",
        "    \n",
        "    for i in range(lstm_layers):\n",
        "      x2_=layers.Bidirectional(layers.GRU(lstm_units,\n",
        "                                       return_sequences=True,\n",
        "                                     kernel_initializer='random_normal',\n",
        "                                     recurrent_initializer='random_normal'))(x2_)\n",
        "    if (att):\n",
        "      x2_=attention()(x2_)\n",
        "    \n",
        "    x2_=Flatten()(x2_)\n",
        "    for i in range(num_layers):\n",
        "        x2_=Dense(hidden_units,activation='relu',kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001))(x2_)\n",
        "        x2_=Dropout(dropOutRate)(x2_)\n",
        "    \n",
        "    x3=Dense(1,activation=\"relu\")(x2_)\n",
        "    \n",
        "    model=keras.Model(inputs=x2_i,outputs=x3)   \n",
        "  \n",
        "    return model"
      ],
      "metadata": {
        "id": "J5akQnDkMerl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAIN"
      ],
      "metadata": {
        "id": "ZvKh1IdhP4ih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load y:"
      ],
      "metadata": {
        "id": "zi-hxP-FP7mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickle_in = open(\"./IDs_test.pickle\",'rb')\n",
        "ids_test= pickle.load(pickle_in)\n",
        "pickle_in.close()\n",
        "\n",
        "pickle_in = open(\"./IDs_train.pickle\",'rb')\n",
        "ids_train= pickle.load(pickle_in)\n",
        "pickle_in.close()\n",
        "\n",
        "pickle_in2 = open('./scores.pickle','rb')\n",
        "labels = pickle.load(pickle_in2)\n",
        "pickle_in2.close()"
      ],
      "metadata": {
        "id": "-9LCTURFP56V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=[]\n",
        "for i in ids_train:\n",
        "  y_train.append(float(labels[i]))\n",
        "\n",
        "y_test=[]\n",
        "for i in ids_test:\n",
        "  y_test.append(float(labels[i]))"
      ],
      "metadata": {
        "id": "JiDmAapLQcz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=np.array(y_train)\n",
        "y_test=np.array(y_test)"
      ],
      "metadata": {
        "id": "YrnMZ7LbQo5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load X:"
      ],
      "metadata": {
        "id": "RMe7jhf4QF5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Uncomment the following commands to load features\n",
        "#h5f = h5py.File('features.h5','r')\n",
        "#X_train = h5f['X_train'][:]\n",
        "#X_test = h5f['X_test'][:]\n",
        "#h5f.close()"
      ],
      "metadata": {
        "id": "MrlppOWUQHiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train function:"
      ],
      "metadata": {
        "id": "YRHlMCvCRJyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Training(X_train,y_train,\n",
        "    dropOutRate=0,batch_size=2,save_Weights=False,save_json=True,\n",
        "    use_log=False,epochs=30, db='koniq', log_path='', lstm_layers_p=2,\n",
        "    lstm_units_p=128, hidden_units_p=512, num_layers_p=2, pooling_type='resnet'):\n",
        "   \n",
        "    time=datetime.datetime.now()  \n",
        "    training_type= pooling_type+'with:'+db\n",
        "\n",
        "   \n",
        "    print(training_type)\n",
        "\n",
        "  \n",
        "    params = {'batch_size': batch_size,\n",
        "              'shuffle': True  \n",
        "              }\n",
        "\n",
        "    loss='mean_squared_error'\n",
        "    print('training generator call done')\n",
        "\n",
        "\n",
        "    #check if there are pretrained model\n",
        "    print('loading model...')\n",
        "\n",
        "    try:\n",
        "      model.load_weights('weights/def_weights.h5')\n",
        "    except:\n",
        "      print(\"No weights available\")\n",
        "    \n",
        "    model = rnn_model(dropOutRate=0.25,lstm_layers=lstm_layers_p,lstm_units=lstm_units_p,hidden_units=hidden_units_p,\n",
        "                         num_layers=num_layers_p)\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    out_model_path =pooling_type+'_'+db\n",
        "        \n",
        "\n",
        "    adam=Adam(lr=0.001)\n",
        "    model.compile(\n",
        "    optimizer=adam,\n",
        "    loss=loss\n",
        "    )\n",
        "    \n",
        "   \n",
        "    callbacks = [\n",
        "     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9, epsilon=0.00001, verbose=1, mode='min'),\n",
        "     EarlyStopping(monitor='val_loss', patience=10, verbose=0,restore_best_weights=True) ]\n",
        "    \n",
        "    training_type= pooling_type+' with '+db\n",
        "    \n",
        "    print('Start training of '+ training_type)\n",
        "   \n",
        "    history=model.fit(x=X_train,y=y_train,\n",
        "                      epochs=epochs,\n",
        "                      callbacks=callbacks\n",
        "                      )\n",
        "    \n",
        "    print('Training is finished weights are generated ...')\n",
        "    pandas.DataFrame(history.history).to_csv(log_path+training_type+'_train.csv', index=False)\n",
        "\n",
        "    if(use_log):\n",
        "        log_file.close()\n",
        "        send_log(log_path+'_train.csv',message= training_type)\n",
        "\n",
        "    #save only weights\n",
        "    if(save_Weights):\n",
        "        model.save_weights(out_model_path+'.h5')\n",
        "    return model"
      ],
      "metadata": {
        "id": "rXfiH7HkROh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Training( batch_size=4,dropOutRate=0.25,epochs=100,lstm_layers_p=3,lstm_units_p=1024,hidden_units_p=1024)"
      ],
      "metadata": {
        "id": "ITHzL_RwS4e0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}